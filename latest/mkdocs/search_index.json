{
    "docs": [
        {
            "location": "/", 
            "text": "BasisFunctionExpansions\n\n\n \n \n\n\nA Julia toolbox for approximation of functions using basis function expansions (BFEs).\n\n\nBFEs are useful when one wants to estimate an arbitrary/unknown/complicated functional relationship between (in the simple case) two variables, \ny\n and \nv\n. In simple linear regression, we might consider a functional relationship \ny = \\phi(v) = \\alpha v + \\beta\n, with parameters \n\\alpha\n and \n\\beta\n. However, if the function \n\\phi\n has an arbitrary nonlinar form, it might be hard to come up with suitable basis functions to use for linear regression. This package provides a set of convenient methods to estimate \n\\phi(v)\n as a linear combination of basis functions, such as radial basis functions, for situations where \nv\n has a single or multiple dimensions.\n\n\n\n\nBasisFunctionExpansions\n\n\nExported functions and types\n\n\nUsage\n\n\nSingle dimension\n\n\nPlotting\n\n\n\n\n\n\nMultiple dimensions\n\n\nNonuniform covariance\n\n\nFull covariance\n\n\n\n\n\n\n\n\n\n\nSelecting the number of basis functions\n\n\nDynamics modeling\n\n\nLPV ARX modeling\n\n\nLPV State-space modeling\n\n\n\n\n\n\nGradients\n\n\nLearn more\n\n\nIndex\n\n\n\n\n\n\nExported functions and types\n\n\n#\n\n\nBasisFunctionExpansions.MultiDiagonalRBFE\n \n \nType\n.\n\n\nA \nMultiDiagonalRBFE\n has different diagonal covariance matrices for all basis functions See also \nMultiUniformRBFE\n, which has the same covariance matrix for all basis functions\n\n\nsource\n\n\n#\n\n\nBasisFunctionExpansions.MultiDiagonalRBFE\n \n \nMethod\n.\n\n\nMultiDiagonalRBFE(v::AbstractVector, nc; normalize=false, coulomb=false)\n\n\n\n\nSupply scheduling signal \nv\n and numer of centers \nnc\n For automatic selection of covariance matrices and centers using K-means.\n\n\nThe keyword \nnormalize\n determines weather or not basis function activations are normalized to sum to one for each datapoint, normalized networks tend to extrapolate better \n\"The normalized radial basis function neural network\" DOI: 10.1109/ICSMC.1998.728118\n\n\nsource\n\n\n#\n\n\nBasisFunctionExpansions.MultiDiagonalRBFE\n \n \nMethod\n.\n\n\nMultiDiagonalRBFE(\u03bc::Matrix, \u03a3::Vector{Vector{Float64}}, activation)\n\n\n\n\nSupply all parameters. \u03a3 is the diagonals of the covariance matrices\n\n\nsource\n\n\n#\n\n\nBasisFunctionExpansions.MultiRBFE\n \n \nType\n.\n\n\nA \nMultiRBFE\n has different diagonal covariance matrices for all basis functions See also \nMultiUniformRBFE\n, which has the same covariance matrix for all basis functions\n\n\nsource\n\n\n#\n\n\nBasisFunctionExpansions.MultiRBFE\n \n \nMethod\n.\n\n\nMultiRBFE(v::AbstractVector, nc; normalize=false, coulomb=false)\n\n\n\n\nSupply scheduling signal \nv\n and numer of centers \nnc\n For automatic selection of covariance matrices and centers using K-means.\n\n\nThe keyword \nnormalize\n determines weather or not basis function activations are normalized to sum to one for each datapoint, normalized networks tend to extrapolate better \n\"The normalized radial basis function neural network\" DOI: 10.1109/ICSMC.1998.728118\n\n\nsource\n\n\n#\n\n\nBasisFunctionExpansions.MultiRBFE\n \n \nMethod\n.\n\n\nMultiRBFE(\u03bc::Matrix, \u03a3::Vector{Vector{Float64}}, activation)\n\n\n\n\nSupply all parameters. \u03a3 is the diagonals of the covariance matrices\n\n\nsource\n\n\n#\n\n\nBasisFunctionExpansions.MultiUniformRBFE\n \n \nType\n.\n\n\nA \nMultiUniformRBFE\n has the same diagonal covariance matrix for all basis functions See also \nMultiDiagonalRBFE\n, which has different covariance matrices for all basis functions\n\n\nsource\n\n\n#\n\n\nBasisFunctionExpansions.MultiUniformRBFE\n \n \nMethod\n.\n\n\nMultiUniformRBFE(v::AbstractVector, Nv::Vector{Int}; normalize=false, coulomb=false)\n\n\n\n\nSupply scheduling signal and number of basis functions For automatic selection of centers and widths\n\n\nThe keyword \nnormalize\n determines weather or not basis function activations are normalized to sum to one for each datapoint, normalized networks tend to extrapolate better \n\"The normalized radial basis function neural network\" DOI: 10.1109/ICSMC.1998.728118\n\n\nsource\n\n\n#\n\n\nBasisFunctionExpansions.MultiUniformRBFE\n \n \nMethod\n.\n\n\nMultiUniformRBFE(\u03bc::Matrix, \u03a3::Vector, activation)\n\n\n\n\nSupply all parameters. \u03a3 is the diagonal of the covariance matrix\n\n\nsource\n\n\n#\n\n\nBasisFunctionExpansions.BasisFunctionApproximation\n \n \nType\n.\n\n\nBasisFunctionApproximation(y::Vector, v, bfe::BasisFunctionExpansion, \u03bb = 0)\n\n\n\n\nPerform parameter identification to identify the Function \ny = \u03d5(v)\n, where \n\u03d5\n is a Basis Function Expansion of type \nbfe\n. \n\u03bb\n is an optional regularization parameter (L\u00b2 regularization).\n\n\nsource\n\n\n#\n\n\nBasisFunctionExpansions.UniformRBFE\n \n \nType\n.\n\n\nA Uniform RBFE has the same variance for all basis functions\n\n\nsource\n\n\n#\n\n\nBasisFunctionExpansions.UniformRBFE\n \n \nMethod\n.\n\n\nUniformRBFE(\u03bc::Vector, \u03c3::Float, activation)\n\n\n\n\nSupply all parameters. OBS! \n\u03c3\n can not be an integer, must be some kind of AbstractFloat\n\n\nsource\n\n\n#\n\n\nBasisFunctionExpansions.UniformRBFE\n \n \nMethod\n.\n\n\nUniformRBFE(v::Vector, Nv::Int; normalize=false, coulomb=false)\n\n\n\n\nSupply scheduling signal and number of basis functions For automatic selection of centers and widths\n\n\nThe keyword \nnormalize\n determines weather or not basis function activations are normalized to sum to one for each datapoint, normalized networks tend to extrapolate better \n\"The normalized radial basis function neural network\" DOI: 10.1109/ICSMC.1998.728118\n\n\nsource\n\n\n#\n\n\nBasisFunctionExpansions.get_centers\n \n \nFunction\n.\n\n\nvc,\u03b3 = get_centers(bounds, Nv, coulomb=false, coulombdims=0)\n\n\n\n\nsource\n\n\n#\n\n\nBasisFunctionExpansions.get_centers_automatic\n \n \nFunction\n.\n\n\nvc,\u03b3 = get_centers_automatic(v::AbstractMatrix, Nv::AbstractVector{Int}, coulomb=false, coulombdims=0)\n\n\n\n\nsource\n\n\n#\n\n\nBasisFunctionExpansions.get_centers_automatic\n \n \nFunction\n.\n\n\nvc,\u03b3 = get_centers_automatic(v::AbstractVector,Nv::Int,coulomb = false)\n\n\n\n\nsource\n\n\n\n\nUsage\n\n\nWe demonstrate typical usage with some examples.\n\n\nThe idea is to create an object representing an expansion. This object contains information regarding the domain of the expansion, which type of basis functions used and how many. These objects are, once created, callable with a scheduling vector/matrix. A call like this returns a vector/matrix of basis function activations.\n\n\nTo reconstruct a signal, a linear combination of basis functions must be estimated. To facilitate this, a second type of object is available: \nBasisFunctionApproximation\n. Once created, \nBasisFunctionApproximation\ns are callable with a scheduling signal and return an estimate of the output. The parameter estimation is performed behind the scenes using standard linear regression (least-squares). An optional regularization parameter can be supplied if needed, see \n?BasisFunctionApproximation\n for help.\n\n\nPlotting functionality requires \nPlots.jl\n\n\n\n\nSingle dimension\n\n\nWe start by simulating a signal \ny\n and a scheduling signal \nv\n. The task is to estimate a function \ny = \\phi(v)\n, where \n\\phi\n is a basis function expansion.\n\n\nN = 1000\nv = linspace(0,10,N) # Scheduling signal\ny = randn(N) # Signal to be approximated\ny = filt(ones(500)/500,[1],y)\n\n\n\n\nNext, we setup the basis function expansion object \nrbf\n and use it to create a reconstruction object \nbfa\n\n\nNv  = 10 # Number of basis functions\nrbf = UniformRBFE(v,Nv, normalize=true) # Approximate using radial basis functions with constant width\nbfa = BasisFunctionApproximation(y,v,rbf,1) # Create approximation object\ny\u0302   = bfa(v) # Reconstruct signal using approximation object\nscatter(v,y, lab=\nSignal\n)\nscatter!(v,y\u0302, lab=\nReconstruction\n)\n\n\n\n\nFor comparison, we can also plot the regular linear regression \ny = \\alpha_0  + \\alpha_1 x + \\alpha_2 x^2... \\alpha_n x^n\n for varying orders of \nn\n.\n\n\nA = v.^(0:3)'\ny\u0302_linreg = [A[:,1:i]*(A[:,1:i]\\y) for i=2:4]\nplot!(v,hcat(y\u0302_linreg...), lab=[\nLinear regression order $i\n for i=1:3]')\n\n\n\n\n\n\nAs we can see from the figure, the linear combination of basis functions forming the reconstruction has learnt the overall structure of the signal \ny\n. To capture more detail, one can try to increase the number of basis functions. The final choice of this number is a tradeoff between reconstruction bias and variance, where a high number of basis functions can model the signal in great detail, but may increase the variance if data is sparse.\n\n\n\n\nPlotting\n\n\nBasisFunctionExpansion\n objects can be plotted if \nPlots.jl\n is loaded. This works for 1 and 2 dimensional expansions only.\n\n\nN   = 200\nv   = linspace(0,10,N)\ny   = 0.1*(v-2).*(v-7) + 0.2randn(N)\nrbf = UniformRBFE(v, 5, normalize = true)\nbfa = BasisFunctionApproximation(y,v,rbf)\n\nscatter(v,y,lab=\nSignal\n,c=:orange, subplot=1, xlabel=\n\\$v\\$\n, size=(600,300))\nplot!(rbf)\nplot!(v,bfa(v),lab=\nReconstruction\n,c=:blue,linewidth=2)\n\n\n\n\n\n\n\n\nMultiple dimensions\n\n\nWe now demonstrate the same thing but with \nv \\in \\mathbf{R}^2\n. To create a nice plot, we let \nv\n form a spiral with increasing radius.\n\n\nusing BasisFunctionExpansions\nN = 1000\nx = linspace(0,4pi,N)\nv = [cos(x) sin(x)].*x # Scheduling signal\ny = randn(N) # Signal to be approximated\ny = filt(ones(500)/500,[1],y)\n\n\n\n\nNow we're creating a two-dimensional basis function expansion using ten functions in each dimension (for a total of 10*10=100 parameters).\n\n\nNv  = [10,10] # Number of basis functions along each dimension\nrbf = MultiUniformRBFE(v,Nv, normalize=true) # Approximate using radial basis functions with constant width (Not isotropic, but all functions have the same diagonal covariance matrix)\nbfa = BasisFunctionApproximation(y,v,rbf,0.0001) # Create approximation object\ny\u0302   = bfa(v) # Reconstruct signal using approximation object\nscatter3d(v[:,1],v[:,2],y, lab=\nSignal\n)\nscatter3d!(v[:,1],v[:,2],y\u0302, lab=\nReconstruction\n)\n\n\n\n\n\n\nTo visualize also the basis functions, we can simply call \nplot!(rbf)\n (the exclamation mark adds to the current plot instead of creating a new one). Below is an example when a 5x5 BFE is visualized using \nplotly\n as backend.\n\n\n\n\n\n\nNonuniform covariance\n\n\nWe can let all centers have different (diagonal) covariance matrices using the type \nMultiDiagonalRBFE\n. In this case, good center locations and covariances are estimated using K-means clustering. With this strategy, we can usually get away with much fewer basis functions compared to a uniform grid. A drawback is that we must know in advance which area of the scheduling signal space is of interest.\n\n\nNc   = 8\nrbf  = MultiDiagonalRBFE(v,Nc, normalize=true)\nbfa  = BasisFunctionApproximation(y,v,rbf,0.0001)\nyhat = bfa(v)\nscatter3d(v[:,1],v[:,2],y, lab=\nSignal\n)\nscatter3d!(v[:,1],v[:,2],yhat, lab=\nReconstruction\n)\n\n\n\n\n\n\nFull covariance\n\n\nFor the type \nMultiRBFE\n The covariance matrix and center locations are esimated using K-means.\n\n\nNc   = 8                            # Number of centers/BFs\nrbf  = MultiRBFE(v,Nc, normalize=true)\nbfa  = BasisFunctionApproximation(y,v,rbf,0.0001)\nyhat = bfa(v)\nscatter3d(v[:,1],v[:,2],y, lab=\nSignal\n)\nscatter3d!(v[:,1],v[:,2],yhat, lab=\nReconstruction\n)\n\n\n\n\n\n\nSelecting the number of basis functions\n\n\nA simple way of choosing the number of basis functions is to plot an L-curve (parameter vs. error). A suitable number is where the kink in the curve occurs, for this example at around 6 basis functions.\n\n\nN    = 200\nv    = linspace(0,10,N)\ny    = 0.1*(v-2).*(v-7) + 0.2randn(N)\nnvec = 2:100\nlcurve = map(nvec) do n\n  rbf = UniformRBFE(v, n, normalize = true)\n  bfa = BasisFunctionApproximation(y,v,rbf)\n  std(y-bfa(v))\nend\n\nplot(nvec, lcurve, yscale=:log10, ylabel=\nRMS Error\n, xlabel=\nNumber of basis functions\n)\n\n\n\n\n\n\n\n\nDynamics modeling\n\n\n#\n\n\nBasisFunctionExpansions.LPVSS\n \n \nType\n.\n\n\nConvenience tyoe for estimation of LPV state-space models\n\n\nsource\n\n\n#\n\n\nBasisFunctionExpansions.LPVSS\n \n \nMethod\n.\n\n\nLPVSS(x, u, nc; normalize=true, \u03bb = 1e-3)\n\n\n\n\nLinear Parameter-Varying State-space model. Estimate a state-space model with varying coefficient matrices \nx(t+1) = A(v)x(t) + B(v)u(t)\n. Internally a \nMultiRBFE\n spanning the space of \nX \u00d7 U\n is used. \nx\n and \nu\n should have time in first dimension. Centers are found automatically using k-means, see \nMultiRBFE\n.\n\n\nExamples\n\n\nusing Plots, BasisFunctionExpansions\nx,xm,u,n,m = BasisFunctionExpansions.testdata(1000)\nnc         = 10 # Number of centers\nmodel      = LPVSS(x, u, nc; normalize=true, \u03bb = 1e-3) # Estimate a model\nxh         = model(x,u) # Form prediction\n\neRMS       = \u221a(mean((xh[1:end-1,:]-x[2:end,:]).^2))\n\nplot(xh[1:end-1,:], lab=\nPrediction\n, c=:red, layout=2)\nplot!(x[2:end,:], lab=\nTrue\n, c=:blue); gui()\neRMS \n= 0.37\n\n# output\n\ntrue\n\n\n\n\nsource\n\n\n#\n\n\nBasisFunctionExpansions.LPVSS\n \n \nMethod\n.\n\n\nLPVSS(x, u, v, nc; normalize=true, \u03bb = 1e-3)\n\n\n\n\nLinear Parameter-Varying State-space model. Estimate a state-space model with varying coefficient matrices \nx(t+1) = A(v)x(t) + B(v)u(t)\n. Internally a \nMultiRBFE\n or \nUniformRBFE\n spanning the space of \nv\n is used, depending on the dimensionality of \nv\n. \nx\n, \nu\n and \nv\n should have time in first dimension. Centers are found automatically using k-means, see \nMultiRBFE\n.\n\n\nExamples\n\n\nusing Plots, BasisFunctionExpansions\nT          = 1000\nx,xm,u,n,m = BasisFunctionExpansions.testdata(T)\nnc         = 4\nv          = 1:T\nmodel      = LPVSS(x, u, v, nc; normalize=true, \u03bb = 1e-3)\nxh         = model(x,u,v)\n\neRMS       = \u221a(mean((xh[1:end-1,:]-x[2:end,:]).^2))\n\nplot(xh[1:end-1,:], lab=\nPrediction\n, c=:red, layout=(2,1))\nplot!(x[2:end,:], lab=\nTrue\n, c=:blue); gui()\neRMS \n= 0.26\n\n# output\n\ntrue\n\n\n\n\nsource\n\n\n#\n\n\nBasisFunctionExpansions.getARXregressor\n \n \nMethod\n.\n\n\ngetARXregressor(y::AbstractVector,u::AbstractVecOrMat, na, nb)\n\n\n\n\nReturns a shortened output signal \ny\n and a regressor matrix \nA\n such that the least-squares ARX model estimate of order \nna,nb\n is \ny\\A\n\n\nReturn a regressor matrix used to fit an ARX model on, e.g., the form \nA(z)y = B(z)f(u)\n with output \ny\n and input \nu\n where the order of autoregression is \nna\n and the order of input moving average is \nnb\n\n\nExample\n\n\nHere we test the model with the Function \nf(u) = \u221a(|u|)\n\n\nA     = [1,2*0.7*1,1] # A(z) coeffs\nB     = [10,5] # B(z) coeffs\nu     = randn(100) # Simulate 100 time steps with Gaussian input\ny     = filt(B,A,sqrt.(abs.(u)))\nyr,A  = getARXregressor(y,u,3,2) # We assume that we know the system order 3,2\nbfe   = MultiUniformRBFE(A,[2,2,4,4,4], normalize=true)\nbfa   = BasisFunctionApproximation(yr,A,bfe, 1e-3)\ne_bfe = \u221a(mean((yr - bfa(A)).^2)) # (0.005174261451622258)\nplot([yr bfa(A)], lab=[\nSignal\n \nPrediction\n])\n\n\n\n\nSee README (\n?BasisFunctionExpansions\n) for more details\n\n\nsource\n\n\n#\n\n\nBasisFunctionExpansions.getARregressor\n \n \nMethod\n.\n\n\ny,A = getARregressor(y::AbstractVector,na::Integer)\n\n\n\n\nReturns a shortened output signal \ny\n and a regressor matrix \nA\n such that the least-squares AR model estimate of order \nna\n is \ny\\A\n\n\nsource\n\n\n#\n\n\nBasisFunctionExpansions.predict\n \n \nMethod\n.\n\n\npredict(model::LPVSS, x, u, v)\n\n\n\n\nReturn a prediction of the output \nx'\n given the state \nx\n, input \nu\n and scheduling parameter \nv\n This function is called when a \nmodel::LPVSS\n object is called like \nmodel(x,u,v)\n\n\nsource\n\n\n#\n\n\nBasisFunctionExpansions.predict\n \n \nMethod\n.\n\n\npredict(model::LPVSS, x::AbstractMatrix, u)\n\n\n\n\nReturn a prediction of the output \nx'\n given the state \nx\n and input \nu\n This function is called when a \nmodel::LPVSS\n object is called like \nmodel(x,u)\n\n\nsource\n\n\n#\n\n\nBasisFunctionExpansions.toeplitz\n \n \nMethod\n.\n\n\ntoeplitz{T}(c::AbstractArray{T},r::AbstractArray{T})\n\n\n\n\nReturns a Toeplitz matrix where \nc\n is the first column and \nr\n is the first row.\n\n\nsource\n\n\n\n\nLPV ARX modeling\n\n\nWe can use basis function expansions for identification of elementary, non-linear dynamics models. Consider the following dynamical system, with a non-linearity on the input \nA(z)y = B(z)\\sqrt{|u|}\n We can simulate this system using the code\n\n\nA = [1,2*0.7*1,1] # A(z) coeffs\nB = [10,5]        # B(z) coeffs\nu = randn(100)    # Simulate 100 time steps with Gaussian input\ny = filt(B,A,sqrt.(abs.(u)))\n\n\n\n\nWe can now try to fit a regular ARX model to this input-output data\n\n\nyr,A  = getARXregressor(y,u,3,2) # We assume that we know the system order 3,2\nx     = A\\yr                     # Fit using standard least-squares\ne_arx = \u221a(mean((yr - A*x).^2))   # Calculate RMS error (4.2553882233771025)\nplot([yr A*x], lab=[\nSignal\n \nARX prediction\n])\n\n\n\n\n\n\nDue to the non-linearity at the input of the system, the linear model fails to fit the data well. Our next attempt is a non-linear model based on BFEs. We select the simplest form of multi-dimensional BFE, \nMultiUniformRBFE\n and further select to cover the state-space with 2 basis functions along each dimension corresponding to \ny\n, and 4 basis functions along each dimension corresponding to \nu\n for a total of 2^2*4^3=256 parameters (4 basis functions is the smallest number that can somewhat accurately fit \n\\sqrt{|u|}\n). The number of parameters in this case is large compared to the number of data points, we will need some regularization to fit this model properly. The regularization choice is made when forming the \nBasisFunctionApproximation\n and the strength is determined by the last argument \n1e-3\n in this case.\n\n\nbfe   = MultiUniformRBFE(A,[2,2,4,4,4], normalize=true)\nbfa   = BasisFunctionApproximation(yr,A,bfe, 1e-3)\ne_bfe = \u221a(mean((yr - bfa(A)).^2)) # (0.005174261451622258)\n\n\n\n\n\n\nThe non-linear model fits the data much better!\n\n\nWe also note that if we knew in advance that the system is linear with a non-linearity on the input, we could do this in a slightly more efficient way by incorporating lagged values of \ny\n directly in the regressor, instead of expanding the lagged values of \ny\n in a BFE. If we knew the exact non-linearity, we could simply transform our measured signal \nu\n and use it as input. With the LPV model, however, we can estimate the shape of the non-linearity.\n\n\n\n\nLPV State-space modeling\n\n\nWe can also estimate a state-space model with varying coefficient matrices, i.e. a model on the form \nx(t+1) = A(v)x(t) + B(v)u(t)\n\n\n\n\nThis is accomplished using the built in convenience type \nLPVSS\n\n\nUnder the hood, the system \nx(t+1) = A(v)x(t) + B(v)u(t), \\quad x(t) \\in \\mathbf{R}^n, u(t) \\in \\mathbf{R}^m\n, which is linear in the parameters of \nA\n and \nB\n, is written on the form \nx_i(t+1) = \\Phi k_i(v) \\quad \\forall i \\in [1,n]\n, where \n\\Phi\n is a regressor matrix consisting of \nx\n and \nu\n, and \nk_i(v) \\in \\mathbf{R}^{n+m}\n are the coefficients to be estimated for each \ni\n.\n\n\nusing Plots, BasisFunctionExpansions\nT          = 1000\nx,xm,u,n,m = BasisFunctionExpansions.testdata(T)\nnc         = 4\nv          = 1:T\nmodel      = LPVSS(x, u, v, nc; normalize=true, \u03bb = 1e-3)\nxh         = model(x,u,v)\n\neRMS       = \u221a(mean((xh[1:end-1,:]-x[2:end,:]).^2))\n\nplot(xh[1:end-1,:], lab=\nPrediction\n, c=:red, layout=(2,1))\nplot!(x[2:end,:], lab=\nTrue\n, c=:blue); gui()\n\n\n\n\n\n\n\n\nGradients\n\n\nBasisFunctionExpansions plays nice with \nReverseDiff.jl\n and \nForwardDiff.jl\n\n\njulia\n using ReverseDiff\njulia\n a = randn(1,2)\njulia\n ReverseDiff.gradient(bfa,a) # bfa here comes from the Multi-dim example\n1\u00d72 Array{Float64,2}:\n 1.29364  -0.536586\n\njulia\n h = 0.0001 # Finite difference for validation\n0.0001\n\njulia\n [(bfa(a+[h 0]) - bfa(a))/h (bfa(a+[0 h]) - bfa(a))/h]\n1\u00d72 Array{Float64,2}:\n 1.29363  -0.536488\n\n\n\n\nNote: for \nForwardDiff.jl\n to work, you have to use \nForwardDiff.jacobian\n instead of  \nForwardDiff.gradient\n.\n\n\nSee \n?ReverseDiff.gradient\n for tips regarding high performance gradient calculation through preallocation of GradientConfig and prerecording of \nbfa\n.\n\n\n\n\nLearn more\n\n\nFunctionality in this package is used in the packages\n\n\n\n\nRobotlib.jl\n\n\nLPVSpectral.jl\n\n\nDynamicMovementPrimitives.jl\n\n\n\n\nAnd in the papers\n\n\n\n\n\"Linear Parameter-Varying Spectral Decomposition\" Bagge Carlson, Fredrik; Robertsson, Anders and Johansson, Rolf (2017) American Control Conference Conference\n\n\n\"Modeling and Identification of Position and Temperature Dependent Friction Phenomena without Temperature Sensing\" Bagge Carlson, Fredrik; Robertsson, Anders and Johansson, Rolf (2015) IEEE/RSJ International Conference on Intelligent Robots and Systems\n\n\n\n\n\n\nIndex\n\n\n\n\nBasisFunctionExpansions.BasisFunctionApproximation\n\n\nBasisFunctionExpansions.LPVSS\n\n\nBasisFunctionExpansions.LPVSS\n\n\nBasisFunctionExpansions.LPVSS\n\n\nBasisFunctionExpansions.MultiDiagonalRBFE\n\n\nBasisFunctionExpansions.MultiDiagonalRBFE\n\n\nBasisFunctionExpansions.MultiDiagonalRBFE\n\n\nBasisFunctionExpansions.MultiRBFE\n\n\nBasisFunctionExpansions.MultiRBFE\n\n\nBasisFunctionExpansions.MultiRBFE\n\n\nBasisFunctionExpansions.MultiUniformRBFE\n\n\nBasisFunctionExpansions.MultiUniformRBFE\n\n\nBasisFunctionExpansions.MultiUniformRBFE\n\n\nBasisFunctionExpansions.UniformRBFE\n\n\nBasisFunctionExpansions.UniformRBFE\n\n\nBasisFunctionExpansions.UniformRBFE\n\n\nBasisFunctionExpansions.getARXregressor\n\n\nBasisFunctionExpansions.getARregressor\n\n\nBasisFunctionExpansions.get_centers\n\n\nBasisFunctionExpansions.get_centers_automatic\n\n\nBasisFunctionExpansions.get_centers_automatic\n\n\nBasisFunctionExpansions.predict\n\n\nBasisFunctionExpansions.predict\n\n\nBasisFunctionExpansions.toeplitz", 
            "title": "Home"
        }, 
        {
            "location": "/#basisfunctionexpansions", 
            "text": "A Julia toolbox for approximation of functions using basis function expansions (BFEs).  BFEs are useful when one wants to estimate an arbitrary/unknown/complicated functional relationship between (in the simple case) two variables,  y  and  v . In simple linear regression, we might consider a functional relationship  y = \\phi(v) = \\alpha v + \\beta , with parameters  \\alpha  and  \\beta . However, if the function  \\phi  has an arbitrary nonlinar form, it might be hard to come up with suitable basis functions to use for linear regression. This package provides a set of convenient methods to estimate  \\phi(v)  as a linear combination of basis functions, such as radial basis functions, for situations where  v  has a single or multiple dimensions.   BasisFunctionExpansions  Exported functions and types  Usage  Single dimension  Plotting    Multiple dimensions  Nonuniform covariance  Full covariance      Selecting the number of basis functions  Dynamics modeling  LPV ARX modeling  LPV State-space modeling    Gradients  Learn more  Index", 
            "title": "BasisFunctionExpansions"
        }, 
        {
            "location": "/#exported-functions-and-types", 
            "text": "#  BasisFunctionExpansions.MultiDiagonalRBFE     Type .  A  MultiDiagonalRBFE  has different diagonal covariance matrices for all basis functions See also  MultiUniformRBFE , which has the same covariance matrix for all basis functions  source  #  BasisFunctionExpansions.MultiDiagonalRBFE     Method .  MultiDiagonalRBFE(v::AbstractVector, nc; normalize=false, coulomb=false)  Supply scheduling signal  v  and numer of centers  nc  For automatic selection of covariance matrices and centers using K-means.  The keyword  normalize  determines weather or not basis function activations are normalized to sum to one for each datapoint, normalized networks tend to extrapolate better  \"The normalized radial basis function neural network\" DOI: 10.1109/ICSMC.1998.728118  source  #  BasisFunctionExpansions.MultiDiagonalRBFE     Method .  MultiDiagonalRBFE(\u03bc::Matrix, \u03a3::Vector{Vector{Float64}}, activation)  Supply all parameters. \u03a3 is the diagonals of the covariance matrices  source  #  BasisFunctionExpansions.MultiRBFE     Type .  A  MultiRBFE  has different diagonal covariance matrices for all basis functions See also  MultiUniformRBFE , which has the same covariance matrix for all basis functions  source  #  BasisFunctionExpansions.MultiRBFE     Method .  MultiRBFE(v::AbstractVector, nc; normalize=false, coulomb=false)  Supply scheduling signal  v  and numer of centers  nc  For automatic selection of covariance matrices and centers using K-means.  The keyword  normalize  determines weather or not basis function activations are normalized to sum to one for each datapoint, normalized networks tend to extrapolate better  \"The normalized radial basis function neural network\" DOI: 10.1109/ICSMC.1998.728118  source  #  BasisFunctionExpansions.MultiRBFE     Method .  MultiRBFE(\u03bc::Matrix, \u03a3::Vector{Vector{Float64}}, activation)  Supply all parameters. \u03a3 is the diagonals of the covariance matrices  source  #  BasisFunctionExpansions.MultiUniformRBFE     Type .  A  MultiUniformRBFE  has the same diagonal covariance matrix for all basis functions See also  MultiDiagonalRBFE , which has different covariance matrices for all basis functions  source  #  BasisFunctionExpansions.MultiUniformRBFE     Method .  MultiUniformRBFE(v::AbstractVector, Nv::Vector{Int}; normalize=false, coulomb=false)  Supply scheduling signal and number of basis functions For automatic selection of centers and widths  The keyword  normalize  determines weather or not basis function activations are normalized to sum to one for each datapoint, normalized networks tend to extrapolate better  \"The normalized radial basis function neural network\" DOI: 10.1109/ICSMC.1998.728118  source  #  BasisFunctionExpansions.MultiUniformRBFE     Method .  MultiUniformRBFE(\u03bc::Matrix, \u03a3::Vector, activation)  Supply all parameters. \u03a3 is the diagonal of the covariance matrix  source  #  BasisFunctionExpansions.BasisFunctionApproximation     Type .  BasisFunctionApproximation(y::Vector, v, bfe::BasisFunctionExpansion, \u03bb = 0)  Perform parameter identification to identify the Function  y = \u03d5(v) , where  \u03d5  is a Basis Function Expansion of type  bfe .  \u03bb  is an optional regularization parameter (L\u00b2 regularization).  source  #  BasisFunctionExpansions.UniformRBFE     Type .  A Uniform RBFE has the same variance for all basis functions  source  #  BasisFunctionExpansions.UniformRBFE     Method .  UniformRBFE(\u03bc::Vector, \u03c3::Float, activation)  Supply all parameters. OBS!  \u03c3  can not be an integer, must be some kind of AbstractFloat  source  #  BasisFunctionExpansions.UniformRBFE     Method .  UniformRBFE(v::Vector, Nv::Int; normalize=false, coulomb=false)  Supply scheduling signal and number of basis functions For automatic selection of centers and widths  The keyword  normalize  determines weather or not basis function activations are normalized to sum to one for each datapoint, normalized networks tend to extrapolate better  \"The normalized radial basis function neural network\" DOI: 10.1109/ICSMC.1998.728118  source  #  BasisFunctionExpansions.get_centers     Function .  vc,\u03b3 = get_centers(bounds, Nv, coulomb=false, coulombdims=0)  source  #  BasisFunctionExpansions.get_centers_automatic     Function .  vc,\u03b3 = get_centers_automatic(v::AbstractMatrix, Nv::AbstractVector{Int}, coulomb=false, coulombdims=0)  source  #  BasisFunctionExpansions.get_centers_automatic     Function .  vc,\u03b3 = get_centers_automatic(v::AbstractVector,Nv::Int,coulomb = false)  source", 
            "title": "Exported functions and types"
        }, 
        {
            "location": "/#usage", 
            "text": "We demonstrate typical usage with some examples.  The idea is to create an object representing an expansion. This object contains information regarding the domain of the expansion, which type of basis functions used and how many. These objects are, once created, callable with a scheduling vector/matrix. A call like this returns a vector/matrix of basis function activations.  To reconstruct a signal, a linear combination of basis functions must be estimated. To facilitate this, a second type of object is available:  BasisFunctionApproximation . Once created,  BasisFunctionApproximation s are callable with a scheduling signal and return an estimate of the output. The parameter estimation is performed behind the scenes using standard linear regression (least-squares). An optional regularization parameter can be supplied if needed, see  ?BasisFunctionApproximation  for help.  Plotting functionality requires  Plots.jl", 
            "title": "Usage"
        }, 
        {
            "location": "/#single-dimension", 
            "text": "We start by simulating a signal  y  and a scheduling signal  v . The task is to estimate a function  y = \\phi(v) , where  \\phi  is a basis function expansion.  N = 1000\nv = linspace(0,10,N) # Scheduling signal\ny = randn(N) # Signal to be approximated\ny = filt(ones(500)/500,[1],y)  Next, we setup the basis function expansion object  rbf  and use it to create a reconstruction object  bfa  Nv  = 10 # Number of basis functions\nrbf = UniformRBFE(v,Nv, normalize=true) # Approximate using radial basis functions with constant width\nbfa = BasisFunctionApproximation(y,v,rbf,1) # Create approximation object\ny\u0302   = bfa(v) # Reconstruct signal using approximation object\nscatter(v,y, lab= Signal )\nscatter!(v,y\u0302, lab= Reconstruction )  For comparison, we can also plot the regular linear regression  y = \\alpha_0  + \\alpha_1 x + \\alpha_2 x^2... \\alpha_n x^n  for varying orders of  n .  A = v.^(0:3)'\ny\u0302_linreg = [A[:,1:i]*(A[:,1:i]\\y) for i=2:4]\nplot!(v,hcat(y\u0302_linreg...), lab=[ Linear regression order $i  for i=1:3]')   As we can see from the figure, the linear combination of basis functions forming the reconstruction has learnt the overall structure of the signal  y . To capture more detail, one can try to increase the number of basis functions. The final choice of this number is a tradeoff between reconstruction bias and variance, where a high number of basis functions can model the signal in great detail, but may increase the variance if data is sparse.", 
            "title": "Single dimension"
        }, 
        {
            "location": "/#plotting", 
            "text": "BasisFunctionExpansion  objects can be plotted if  Plots.jl  is loaded. This works for 1 and 2 dimensional expansions only.  N   = 200\nv   = linspace(0,10,N)\ny   = 0.1*(v-2).*(v-7) + 0.2randn(N)\nrbf = UniformRBFE(v, 5, normalize = true)\nbfa = BasisFunctionApproximation(y,v,rbf)\n\nscatter(v,y,lab= Signal ,c=:orange, subplot=1, xlabel= \\$v\\$ , size=(600,300))\nplot!(rbf)\nplot!(v,bfa(v),lab= Reconstruction ,c=:blue,linewidth=2)", 
            "title": "Plotting"
        }, 
        {
            "location": "/#multiple-dimensions", 
            "text": "We now demonstrate the same thing but with  v \\in \\mathbf{R}^2 . To create a nice plot, we let  v  form a spiral with increasing radius.  using BasisFunctionExpansions\nN = 1000\nx = linspace(0,4pi,N)\nv = [cos(x) sin(x)].*x # Scheduling signal\ny = randn(N) # Signal to be approximated\ny = filt(ones(500)/500,[1],y)  Now we're creating a two-dimensional basis function expansion using ten functions in each dimension (for a total of 10*10=100 parameters).  Nv  = [10,10] # Number of basis functions along each dimension\nrbf = MultiUniformRBFE(v,Nv, normalize=true) # Approximate using radial basis functions with constant width (Not isotropic, but all functions have the same diagonal covariance matrix)\nbfa = BasisFunctionApproximation(y,v,rbf,0.0001) # Create approximation object\ny\u0302   = bfa(v) # Reconstruct signal using approximation object\nscatter3d(v[:,1],v[:,2],y, lab= Signal )\nscatter3d!(v[:,1],v[:,2],y\u0302, lab= Reconstruction )   To visualize also the basis functions, we can simply call  plot!(rbf)  (the exclamation mark adds to the current plot instead of creating a new one). Below is an example when a 5x5 BFE is visualized using  plotly  as backend.", 
            "title": "Multiple dimensions"
        }, 
        {
            "location": "/#nonuniform-covariance", 
            "text": "We can let all centers have different (diagonal) covariance matrices using the type  MultiDiagonalRBFE . In this case, good center locations and covariances are estimated using K-means clustering. With this strategy, we can usually get away with much fewer basis functions compared to a uniform grid. A drawback is that we must know in advance which area of the scheduling signal space is of interest.  Nc   = 8\nrbf  = MultiDiagonalRBFE(v,Nc, normalize=true)\nbfa  = BasisFunctionApproximation(y,v,rbf,0.0001)\nyhat = bfa(v)\nscatter3d(v[:,1],v[:,2],y, lab= Signal )\nscatter3d!(v[:,1],v[:,2],yhat, lab= Reconstruction )", 
            "title": "Nonuniform covariance"
        }, 
        {
            "location": "/#full-covariance", 
            "text": "For the type  MultiRBFE  The covariance matrix and center locations are esimated using K-means.  Nc   = 8                            # Number of centers/BFs\nrbf  = MultiRBFE(v,Nc, normalize=true)\nbfa  = BasisFunctionApproximation(y,v,rbf,0.0001)\nyhat = bfa(v)\nscatter3d(v[:,1],v[:,2],y, lab= Signal )\nscatter3d!(v[:,1],v[:,2],yhat, lab= Reconstruction )", 
            "title": "Full covariance"
        }, 
        {
            "location": "/#selecting-the-number-of-basis-functions", 
            "text": "A simple way of choosing the number of basis functions is to plot an L-curve (parameter vs. error). A suitable number is where the kink in the curve occurs, for this example at around 6 basis functions.  N    = 200\nv    = linspace(0,10,N)\ny    = 0.1*(v-2).*(v-7) + 0.2randn(N)\nnvec = 2:100\nlcurve = map(nvec) do n\n  rbf = UniformRBFE(v, n, normalize = true)\n  bfa = BasisFunctionApproximation(y,v,rbf)\n  std(y-bfa(v))\nend\n\nplot(nvec, lcurve, yscale=:log10, ylabel= RMS Error , xlabel= Number of basis functions )", 
            "title": "Selecting the number of basis functions"
        }, 
        {
            "location": "/#dynamics-modeling", 
            "text": "#  BasisFunctionExpansions.LPVSS     Type .  Convenience tyoe for estimation of LPV state-space models  source  #  BasisFunctionExpansions.LPVSS     Method .  LPVSS(x, u, nc; normalize=true, \u03bb = 1e-3)  Linear Parameter-Varying State-space model. Estimate a state-space model with varying coefficient matrices  x(t+1) = A(v)x(t) + B(v)u(t) . Internally a  MultiRBFE  spanning the space of  X \u00d7 U  is used.  x  and  u  should have time in first dimension. Centers are found automatically using k-means, see  MultiRBFE .  Examples  using Plots, BasisFunctionExpansions\nx,xm,u,n,m = BasisFunctionExpansions.testdata(1000)\nnc         = 10 # Number of centers\nmodel      = LPVSS(x, u, nc; normalize=true, \u03bb = 1e-3) # Estimate a model\nxh         = model(x,u) # Form prediction\n\neRMS       = \u221a(mean((xh[1:end-1,:]-x[2:end,:]).^2))\n\nplot(xh[1:end-1,:], lab= Prediction , c=:red, layout=2)\nplot!(x[2:end,:], lab= True , c=:blue); gui()\neRMS  = 0.37\n\n# output\n\ntrue  source  #  BasisFunctionExpansions.LPVSS     Method .  LPVSS(x, u, v, nc; normalize=true, \u03bb = 1e-3)  Linear Parameter-Varying State-space model. Estimate a state-space model with varying coefficient matrices  x(t+1) = A(v)x(t) + B(v)u(t) . Internally a  MultiRBFE  or  UniformRBFE  spanning the space of  v  is used, depending on the dimensionality of  v .  x ,  u  and  v  should have time in first dimension. Centers are found automatically using k-means, see  MultiRBFE .  Examples  using Plots, BasisFunctionExpansions\nT          = 1000\nx,xm,u,n,m = BasisFunctionExpansions.testdata(T)\nnc         = 4\nv          = 1:T\nmodel      = LPVSS(x, u, v, nc; normalize=true, \u03bb = 1e-3)\nxh         = model(x,u,v)\n\neRMS       = \u221a(mean((xh[1:end-1,:]-x[2:end,:]).^2))\n\nplot(xh[1:end-1,:], lab= Prediction , c=:red, layout=(2,1))\nplot!(x[2:end,:], lab= True , c=:blue); gui()\neRMS  = 0.26\n\n# output\n\ntrue  source  #  BasisFunctionExpansions.getARXregressor     Method .  getARXregressor(y::AbstractVector,u::AbstractVecOrMat, na, nb)  Returns a shortened output signal  y  and a regressor matrix  A  such that the least-squares ARX model estimate of order  na,nb  is  y\\A  Return a regressor matrix used to fit an ARX model on, e.g., the form  A(z)y = B(z)f(u)  with output  y  and input  u  where the order of autoregression is  na  and the order of input moving average is  nb  Example  Here we test the model with the Function  f(u) = \u221a(|u|)  A     = [1,2*0.7*1,1] # A(z) coeffs\nB     = [10,5] # B(z) coeffs\nu     = randn(100) # Simulate 100 time steps with Gaussian input\ny     = filt(B,A,sqrt.(abs.(u)))\nyr,A  = getARXregressor(y,u,3,2) # We assume that we know the system order 3,2\nbfe   = MultiUniformRBFE(A,[2,2,4,4,4], normalize=true)\nbfa   = BasisFunctionApproximation(yr,A,bfe, 1e-3)\ne_bfe = \u221a(mean((yr - bfa(A)).^2)) # (0.005174261451622258)\nplot([yr bfa(A)], lab=[ Signal   Prediction ])  See README ( ?BasisFunctionExpansions ) for more details  source  #  BasisFunctionExpansions.getARregressor     Method .  y,A = getARregressor(y::AbstractVector,na::Integer)  Returns a shortened output signal  y  and a regressor matrix  A  such that the least-squares AR model estimate of order  na  is  y\\A  source  #  BasisFunctionExpansions.predict     Method .  predict(model::LPVSS, x, u, v)  Return a prediction of the output  x'  given the state  x , input  u  and scheduling parameter  v  This function is called when a  model::LPVSS  object is called like  model(x,u,v)  source  #  BasisFunctionExpansions.predict     Method .  predict(model::LPVSS, x::AbstractMatrix, u)  Return a prediction of the output  x'  given the state  x  and input  u  This function is called when a  model::LPVSS  object is called like  model(x,u)  source  #  BasisFunctionExpansions.toeplitz     Method .  toeplitz{T}(c::AbstractArray{T},r::AbstractArray{T})  Returns a Toeplitz matrix where  c  is the first column and  r  is the first row.  source", 
            "title": "Dynamics modeling"
        }, 
        {
            "location": "/#lpv-arx-modeling", 
            "text": "We can use basis function expansions for identification of elementary, non-linear dynamics models. Consider the following dynamical system, with a non-linearity on the input  A(z)y = B(z)\\sqrt{|u|}  We can simulate this system using the code  A = [1,2*0.7*1,1] # A(z) coeffs\nB = [10,5]        # B(z) coeffs\nu = randn(100)    # Simulate 100 time steps with Gaussian input\ny = filt(B,A,sqrt.(abs.(u)))  We can now try to fit a regular ARX model to this input-output data  yr,A  = getARXregressor(y,u,3,2) # We assume that we know the system order 3,2\nx     = A\\yr                     # Fit using standard least-squares\ne_arx = \u221a(mean((yr - A*x).^2))   # Calculate RMS error (4.2553882233771025)\nplot([yr A*x], lab=[ Signal   ARX prediction ])   Due to the non-linearity at the input of the system, the linear model fails to fit the data well. Our next attempt is a non-linear model based on BFEs. We select the simplest form of multi-dimensional BFE,  MultiUniformRBFE  and further select to cover the state-space with 2 basis functions along each dimension corresponding to  y , and 4 basis functions along each dimension corresponding to  u  for a total of 2^2*4^3=256 parameters (4 basis functions is the smallest number that can somewhat accurately fit  \\sqrt{|u|} ). The number of parameters in this case is large compared to the number of data points, we will need some regularization to fit this model properly. The regularization choice is made when forming the  BasisFunctionApproximation  and the strength is determined by the last argument  1e-3  in this case.  bfe   = MultiUniformRBFE(A,[2,2,4,4,4], normalize=true)\nbfa   = BasisFunctionApproximation(yr,A,bfe, 1e-3)\ne_bfe = \u221a(mean((yr - bfa(A)).^2)) # (0.005174261451622258)   The non-linear model fits the data much better!  We also note that if we knew in advance that the system is linear with a non-linearity on the input, we could do this in a slightly more efficient way by incorporating lagged values of  y  directly in the regressor, instead of expanding the lagged values of  y  in a BFE. If we knew the exact non-linearity, we could simply transform our measured signal  u  and use it as input. With the LPV model, however, we can estimate the shape of the non-linearity.", 
            "title": "LPV ARX modeling"
        }, 
        {
            "location": "/#lpv-state-space-modeling", 
            "text": "We can also estimate a state-space model with varying coefficient matrices, i.e. a model on the form  x(t+1) = A(v)x(t) + B(v)u(t)   This is accomplished using the built in convenience type  LPVSS  Under the hood, the system  x(t+1) = A(v)x(t) + B(v)u(t), \\quad x(t) \\in \\mathbf{R}^n, u(t) \\in \\mathbf{R}^m , which is linear in the parameters of  A  and  B , is written on the form  x_i(t+1) = \\Phi k_i(v) \\quad \\forall i \\in [1,n] , where  \\Phi  is a regressor matrix consisting of  x  and  u , and  k_i(v) \\in \\mathbf{R}^{n+m}  are the coefficients to be estimated for each  i .  using Plots, BasisFunctionExpansions\nT          = 1000\nx,xm,u,n,m = BasisFunctionExpansions.testdata(T)\nnc         = 4\nv          = 1:T\nmodel      = LPVSS(x, u, v, nc; normalize=true, \u03bb = 1e-3)\nxh         = model(x,u,v)\n\neRMS       = \u221a(mean((xh[1:end-1,:]-x[2:end,:]).^2))\n\nplot(xh[1:end-1,:], lab= Prediction , c=:red, layout=(2,1))\nplot!(x[2:end,:], lab= True , c=:blue); gui()", 
            "title": "LPV State-space modeling"
        }, 
        {
            "location": "/#gradients", 
            "text": "BasisFunctionExpansions plays nice with  ReverseDiff.jl  and  ForwardDiff.jl  julia  using ReverseDiff\njulia  a = randn(1,2)\njulia  ReverseDiff.gradient(bfa,a) # bfa here comes from the Multi-dim example\n1\u00d72 Array{Float64,2}:\n 1.29364  -0.536586\n\njulia  h = 0.0001 # Finite difference for validation\n0.0001\n\njulia  [(bfa(a+[h 0]) - bfa(a))/h (bfa(a+[0 h]) - bfa(a))/h]\n1\u00d72 Array{Float64,2}:\n 1.29363  -0.536488  Note: for  ForwardDiff.jl  to work, you have to use  ForwardDiff.jacobian  instead of   ForwardDiff.gradient .  See  ?ReverseDiff.gradient  for tips regarding high performance gradient calculation through preallocation of GradientConfig and prerecording of  bfa .", 
            "title": "Gradients"
        }, 
        {
            "location": "/#learn-more", 
            "text": "Functionality in this package is used in the packages   Robotlib.jl  LPVSpectral.jl  DynamicMovementPrimitives.jl   And in the papers   \"Linear Parameter-Varying Spectral Decomposition\" Bagge Carlson, Fredrik; Robertsson, Anders and Johansson, Rolf (2017) American Control Conference Conference  \"Modeling and Identification of Position and Temperature Dependent Friction Phenomena without Temperature Sensing\" Bagge Carlson, Fredrik; Robertsson, Anders and Johansson, Rolf (2015) IEEE/RSJ International Conference on Intelligent Robots and Systems", 
            "title": "Learn more"
        }, 
        {
            "location": "/#index", 
            "text": "BasisFunctionExpansions.BasisFunctionApproximation  BasisFunctionExpansions.LPVSS  BasisFunctionExpansions.LPVSS  BasisFunctionExpansions.LPVSS  BasisFunctionExpansions.MultiDiagonalRBFE  BasisFunctionExpansions.MultiDiagonalRBFE  BasisFunctionExpansions.MultiDiagonalRBFE  BasisFunctionExpansions.MultiRBFE  BasisFunctionExpansions.MultiRBFE  BasisFunctionExpansions.MultiRBFE  BasisFunctionExpansions.MultiUniformRBFE  BasisFunctionExpansions.MultiUniformRBFE  BasisFunctionExpansions.MultiUniformRBFE  BasisFunctionExpansions.UniformRBFE  BasisFunctionExpansions.UniformRBFE  BasisFunctionExpansions.UniformRBFE  BasisFunctionExpansions.getARXregressor  BasisFunctionExpansions.getARregressor  BasisFunctionExpansions.get_centers  BasisFunctionExpansions.get_centers_automatic  BasisFunctionExpansions.get_centers_automatic  BasisFunctionExpansions.predict  BasisFunctionExpansions.predict  BasisFunctionExpansions.toeplitz", 
            "title": "Index"
        }
    ]
}